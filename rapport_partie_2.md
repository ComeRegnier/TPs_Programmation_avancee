# Introduction

Ce rapport explore l'utilisation de la m√©thode de Monte Carlo (MC) pour calculer œÄ en exploitant le parall√©lisme sur des architectures √† m√©moire partag√©e et distribu√©e. Apr√®s une pr√©sentation de l‚Äôalgorithme s√©quentiel, nous √©tudions des variantes parall√®les (it√©ration parall√®le, ma√Ætre-esclave) et analysons deux impl√©mentations Java.

Enfin, nous √©tendons l‚Äô√©tude aux environnements √† m√©moire distribu√©e et au parall√©lisme sur plusieurs machines, en √©valuant les performances des diff√©rentes approches. Ce travail vise √† fournir une vue synth√©tique et claire des strat√©gies et r√©sultats obtenus.

Ce rapport a √©t√© en partie r√©dig√© par ChatGPT, dans le but de le simplifier et de le rendre le plus clair et concis possible.

# I. Monte Carlo pour calculer œÄ

La m√©thode de Monte Carlo repose sur une estimation probabiliste pour approximer œÄ √† partir de tirages al√©atoires.

Soit AD l‚Äôaire d‚Äôun  de disque de rayon ( r = 1 ) :
$$
A_{\text{D}} = \frac{\pi r^2}{4} = \frac{\pi}{4}
$$

Le disque est inscrit dans un carr√© de c√¥t√© ( r = 1 ), dont l‚Äôaire est :
$$
A_c = r^2 = 1
$$

On consid√®re un point ( X_p (x_p, y_p) ) g√©n√©r√© al√©atoirement dans ce carr√©, o√π ( x_p ) et ( y_p ) suivent la loi uniforme ( U(0, 1) ).

La probabilit√© que ( X_p ) appartienne au  de disque est donn√©e par :
$$
P = \frac{A_{\text{D}}}{A_c} = \frac{\pi}{4}
$$

-------------------- 

![Monte Carlo](./images/figure_monte_carlo-1.png)

--------------------

Pour estimer cette probabilit√©, on effectue n tot tirages al√©atoires. Soit n cible le nombre de points qui satisfont la condition x p 2 + y p 2 ‚â§ 1 , c‚Äôest-√†-dire les points situ√©s dans le quart de disque.

Si n tot est suffisamment grand, par la loi des grands nombres, la fr√©quence observ√©e n cible / n tot converge vers la probabilit√© P , soit :
P = n cible n tot ‚âà œÄ 4

On peut ainsi en d√©duire une approximation de œÄ :
œÄ ‚âà 4 ‚ãÖ n cible n tot

Ainsi, plus n tot augmente, plus l'estimation de œÄ se pr√©cise.

## II. Algorithme et parall√©lisation

### A. It√©ration parall√®le

L‚Äôalgorithme de Monte Carlo peut √™tre parall√©lis√© en distribuant les tirages sur plusieurs t√¢ches ind√©pendantes. Voici l'algorithme s√©quentiel de base :

#### Algorithme de base

```c
n_cible = 0;
for (p = 0; n_tot > 0; n_tot--) {
    x_p = rand();  // G√©n√©rer un nombre al√©atoire entre ]0,1[
    y_p = rand();
    if ((x_p * x_p + y_p * y_p) < 1) {
        n_cible++;
    }
}
pi = 4 * n_cible / n_tot;
```

Dans cette version, tout est ex√©cut√© s√©quentiellement. Pour parall√©liser ce code, il faut identifier les t√¢ches et leurs d√©pendances.
Identification des t√¢ches

    T√¢che principale (T0) : Tirer et compter les n_tot points.

    T√¢che secondaire (T1) : Calculer œÄœÄ apr√®s la collecte de n_cible :

D√©pendances et parall√©lisme

    D√©pendances :
        T1 d√©pend de T0 : œÄœÄ ne peut √™tre calcul√© qu‚Äôapr√®s avoir obtenu n_cible.
        T0p2 d√©pend de T0p1 : Un point doit √™tre g√©n√©r√© avant sa v√©rification.

    Ind√©pendances parall√®les :
        Les g√©n√©rations de points (T0p1) peuvent √™tre effectu√©es simultan√©ment, chaque tirage √©tant ind√©pendant.
        Les v√©rifications (T0p2) peuvent aussi √™tre parall√©lis√©es, mais n√©cessitent une gestion s√©curis√©e de n_cible pour √©viter des conflits d‚Äôacc√®s.

Algorithme parall√®le

L‚Äôalgorithme parall√®le repose sur une fonction d√©di√©e TirerPoint() pour g√©n√©rer et √©valuer les points.

TirerPoint() est ind√©pendante et permet donc d'√©xecuter chaque tirage sur plusieurs threads sans d√©pendance entre eux.

# B. Paradigme Master/Worker

Le mod√®le **Master/Worker** repose sur une division du travail en unit√©s ind√©pendantes, chacune trait√©e par un **Worker** (processus ou thread). Chaque Worker effectue un certain nombre de tirages al√©atoires, et un processus principal, appel√© **Master**, agr√®ge les r√©sultats pour calculer œÄ.

## 1. Principe du mod√®le

- Le travail total (`n_tot` tirages) est r√©parti √©quitablement entre `n_workers`.
- Chaque Worker ex√©cute une fonction ind√©pendante pour traiter sa part des tirages.
- Le Master collecte les r√©sultats des Workers et calcule l'estimation finale de œÄ.

Ce mod√®le permet d'optimiser l'utilisation des ressources tout en minimisant les conflits d'acc√®s m√©moire.

---

## 2. Algorithme Master/Worker

### **Fonctions principales**

```cpp
function TirerPoint() {
    x_p = rand();  // G√©n√©rer un nombre al√©atoire dans ]0,1[
    y_p = rand();
    return ((x_p * x_p + y_p * y_p) < 1);
}

function MCWorker(n_charge) {
    n_cible_partiel = 0;
    for (p = 0; n_charge > 0; n_charge--) {
        if (TirerPoint()) {
            n_cible_partiel += 1;
        }
    }
    return n_cible_partiel;
}
```
- TirerPoint() : g√©n√®re un point al√©atoire et v√©rifie s'il appartient au quart de disque.
- MCWorker(n_charge) : effectue n_charge tirages et compte ceux appartenant au quart de disque.

```cpp
n_charge = n_tot / n_workers;
ncibles = [NULL * n_workers];

parallel for (worker = 0; worker < n_workers; worker++) {
    ncibles[worker] = MCWorker(n_charge);
}

n_cible = sum(ncibles);
pi = 4 * n_cible / n_tot;
```

- Chaque Worker traite n_charge = n_tot / n_workers points.
- Le tableau ncibles stocke les r√©sultats de chaque Worker.
- Une fois le calcul termin√©, le Master additionne les valeurs et estime œÄ.

## 3. Gestion des ressources et parall√©lisation
√âlimination des conflits d'acc√®s

Contrairement √† l‚Äôit√©ration parall√®le, o√π plusieurs threads modifiaient une variable partag√©e (n_cible), ici :

    Chaque Worker poss√®de son propre compteur local (n_cible_partiel), √©vitant ainsi les conflits.
    Le seul √©l√©ment partag√© est le tableau ncibles, mais chaque Worker √©crit dans une case distincte.

Optimisation et scalabilit√©

    R√©duction des conflits : aucun verrouillage n√©cessaire, am√©liorant l‚Äôefficacit√©.
    R√©partition √©quilibr√©e : chaque Worker traite une charge √©quivalente, ce qui optimise l'utilisation des ressources.
    Adaptabilit√© aux architectures distribu√©es : le mod√®le fonctionne aussi bien sur des architectures multi-c≈ìurs que sur des syst√®mes distribu√©s.


# 4. Avantages du mod√®le Master/Worker

| **Crit√®re**         | **Avantage** |
|---------------------|-------------|
| **Synchronisation** | R√©duction des conflits gr√¢ce √† des compteurs locaux |
| **Efficacit√©**      | Ex√©cution parall√®le optimis√©e sans acc√®s concurrent √† une variable critique |
| **Scalabilit√©**     | Supporte un grand nombre de Workers sans perte de performance |
| **Modularit√©**      | Adaptable aux architectures distribu√©es, avec des Workers sur diff√©rentes machines |

# III. Mise en ≈ìuvre sur Machine

Nous allons maintenant examiner deux impl√©mentations pratiques de la m√©thode de Monte Carlo pour le calcul de œÄ. L‚Äôobjectif est d‚Äôanalyser leur structure et leur approche de parall√©lisation :

- Identifier le mod√®le de programmation parall√®le utilis√© dans chaque code ainsi que le paradigme suivi (it√©ration parall√®le ou Master/Worker).
- V√©rifier si ces impl√©mentations correspondent aux algorithmes propos√©s en **partie II**.

Dans la **partie IV**, nous effectuerons une analyse d√©taill√©e de chaque code en √©valuant leur **scalabilit√© forte et faible**.

---

## A. Analyse de *Assignment102*

L‚Äôimpl√©mentation *Assignment102* utilise l‚ÄôAPI **Concurrent** pour parall√©liser les calculs n√©cessaires √† l‚Äôestimation de œÄ avec la m√©thode de Monte Carlo. Voici les principaux √©l√©ments analys√©s :

### 1. Structure et API utilis√©e

#### Gestion des threads :
- Le code utilise `ExecutorService` avec un **pool de threads adaptatif** (`newWorkStealingPool`), exploitant efficacement les c≈ìurs disponibles sur la machine.
- Chaque tirage (g√©n√©ration d‚Äôun point al√©atoire) est ex√©cut√© dans une t√¢che ind√©pendante via `Runnable`.

#### Synchronisation avec AtomicInteger :
- La variable partag√©e `nAtomSuccess`, qui compte le nombre de points dans le quart de disque, est prot√©g√©e par un compteur atomique (`AtomicInteger`) pour √©viter les **conflits d‚Äôacc√®s** entre threads.

---

### 2. Mod√®le de programmation parall√®le et paradigme

| **Aspect**        | **D√©tails** |
|------------------|------------|
| **Mod√®le utilis√©** | **It√©ration parall√®le** : chaque tirage correspond √† une t√¢che ind√©pendante soumise au pool de threads. |
| **Paradigme**     | Suit le mod√®le d‚Äôit√©ration parall√®le d√©fini en **partie II.A**. Chaque t√¢che effectue un tirage de mani√®re ind√©pendante, sans d√©pendances entre elles. |

---

### 3. Lien avec notre pseudo-code

L‚Äôimpl√©mentation correspond globalement au pseudo-code d‚Äô**it√©ration parall√®le**, avec les adaptations suivantes :

- Le compteur `n_cible` est remplac√© par un **AtomicInteger** pour g√©rer les **sections critiques**.
- Le d√©couplage des threads est **g√©r√© par l‚ÄôAPI `ExecutorService`**.

---


## B. Analyse de *Pi.java*

L‚Äôimpl√©mentation *Pi.java* repose sur l‚Äôutilisation des **Futures et Callables** pour parall√©liser le calcul de œÄ √† l‚Äôaide de la m√©thode de Monte Carlo.

### 1. Qu‚Äôest-ce qu‚Äôun Future ?  
Un **Future** est un conteneur pour un r√©sultat **calcul√© de mani√®re asynchrone**. Il permet de :

- **Soumettre une t√¢che** : lorsqu‚Äôun thread ex√©cute une t√¢che, son r√©sultat est **stock√©** dans un Future.
- **R√©cup√©rer le r√©sultat** : l‚Äôappel √† `.get()` r√©cup√®re la valeur, mais **bloque** tant que le calcul n‚Äôest pas termin√©.  
  ‚Üí Cela introduit une **barri√®re implicite** qui synchronise les r√©sultats des diff√©rents threads.
- **V√©rifier l‚Äô√©tat d‚Äôex√©cution** : un `Future` peut indiquer si une t√¢che est **termin√©e ou a √©chou√©**.

**Pourquoi l‚Äôutiliser ici ?**  
Les **Futures** garantissent que chaque r√©sultat partiel est **pr√™t avant l‚Äôagr√©gation**, permettant une synchronisation **optimale** entre les threads.

---

![Diagramme de Classe](./images/DiagrammeClasse.png)

---

### 2. Mod√®le de programmation parall√®le et paradigme

| **Aspect**        | **D√©tails** |
|------------------|------------|
| **Mod√®le utilis√©** | **Master/Worker** : le Master cr√©e des Workers pour effectuer les calculs et regroupe leurs r√©sultats √† l‚Äôaide des **Futures**. |
| **Paradigme**     | Bas√© sur des **t√¢ches**, avec une gestion explicite via **Callables**. |

---

### 3. Structure et API utilis√©e

#### üîπ Parall√©lisation avec des Callables :
- Chaque **Worker** est un `Callable<Long>` qui ex√©cute un sous-ensemble du calcul total.
- Il **compte les points dans le quart de disque** pour un certain nombre d‚Äôit√©rations.
- Ces Callables sont ex√©cut√©s par un **pool de threads fixe** (`FixedThreadPool`).

#### üîπ Gestion des r√©sultats avec des Futures :
- Lorsqu‚Äôun `Callable` est soumis au pool de threads, il retourne un **Future<Long>**.
- L‚Äôappel √† `.get()` bloque le thread principal jusqu‚Äô√† ce que **tous les r√©sultats** soient pr√™ts.
- Une fois collect√©s, ces r√©sultats sont **agr√©g√©s** pour calculer œÄ.

---

### 4. Lien avec notre pseudo-code

L'algorithme suit fid√®lement le mod√®le **Master/Worker** d√©crit en **partie II.B** :

| **√âl√©ment**       | **Correspondance dans le code** |
|------------------|--------------------------------|
| **Master**       | G√®re la r√©partition des t√¢ches et l‚Äôagr√©gation des r√©sultats. |
| **Workers**      | Ex√©cut√©s via des `Callables`, chaque Worker applique la m√©thode `MCWorker()`. |
| **Division**     | Le Master r√©partit **√©quitablement** les tirages (`n_charge`) entre les Workers. |

---

### 5. Comparaison avec *Assignment102*

| **Crit√®re**           | **Pi.java (Master/Worker)** | **Assignment102 (It√©ration parall√®le)** |
|----------------------|---------------------------|----------------------------------|
| **Isolation des calculs** | Chaque Worker g√®re ses propres donn√©es. | Acc√®s concurrent √† une variable atomique. |
| **Synchronisation** | R√©duite √† l‚Äôagr√©gation finale (moins co√ªteux). | Synchronisation fr√©quente via `AtomicInteger`. |
| **Gestion des threads** | Callables et `FixedThreadPool`. | `ExecutorService` avec `Runnable`. |
| **Performance attendue** | Plus efficace pour un grand nombre de threads. | Risque de **goulots d‚Äô√©tranglement** d√ª aux acc√®s atomiques. |

---

üîé **Conclusion g√©n√©rale**  
L‚Äôimpl√©mentation *Pi.java* est **plus efficace** qu‚Äô*Assignment102* car elle :
1. **R√©duit la synchronisation co√ªteuse** (seule l‚Äôagr√©gation des r√©sultats est bloquante).
2. **Minimise les acc√®s concurrents** gr√¢ce √† l‚Äô**isolation des Workers**.
3. **Optimise la gestion des threads** via les **Futures**, permettant une meilleure scalabilit√©.

On peut donc **s‚Äôattendre √† de meilleures performances**, **surtout sur des machines multic≈ìurs** et avec **un grand nombre de points et de threads**. 

# IV. √âvaluations et tests de performances

La partie suivante contient des √©l√©ments d'un autre rapport que j'√©cris en parall√®le de celui-ci, qui concerne le module de Qualit√© de D√©veloppement en troisi√®me ann√©e de BUT informatique.

L'ordinateur qui a r√©alis√© ces calculs poss√®de les sp√©cifications suivantes :

- **Processeur** : 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz
- **8 c≈ìurs physiques**
- **16 c≈ìurs logiques**

Notez que les r√©sultats des tests qui suivent ne seront pas les m√™mes selon l'architecture mat√©rielle sur lesquels ils ont √©t√© effectu√©s.

### A. Programme de calcul de performance

Le script **PerformanceTester.java** teste diff√©rentes impl√©mentations de Monte Carlo pour calculer œÄ en s'appuyant sur une interface standardis√©e **MonteCarloImplementation**. Chaque impl√©mentation doit fournir deux m√©thodes : `execute(int totalPoints, int numCores)` et `getName()`.

Le programme r√©cup√®re les donn√©es de tests (nombre de c≈ìurs, de points, et r√©p√©titions) depuis un fichier CSV. Il ex√©cute ensuite les tests sur chaque impl√©mentation puis enregistre les r√©sultats (temps d'ex√©cution, approximation de œÄ, erreur relative) dans un fichier `resultats.csv`.

L'outil est modulaire, ce qui permet d'ajouter facilement de nouvelles versions de Monte Carlo √† tester. Il automatise √©galement l'√©valuation des performances pour diff√©rents sc√©narios et configurations.

---

### B. Tests de Scalabilit√©

Les tests de scalabilit√© servent √† √©valuer la performance des diff√©rentes impl√©mentations en fonction du nombre de c≈ìurs utilis√©s, selon deux approches : **scalabilit√© forte** et **scalabilit√© faible**.

##### 1. **Scalabilit√© Forte**

La **scalabilit√© forte** mesure la capacit√© d'un programme √† r√©duire son temps d'ex√©cution lorsque le nombre de c≈ìurs augmente, tout en maintenant la charge de travail totale constante. Elle √©value comment le programme exploite efficacement les ressources suppl√©mentaires.

On la mesure √† l‚Äôaide du **speedup**, d√©fini comme :

\[
\text{Speedup} = \frac{\text{Temps\_1\_c≈ìur}}{\text{Temps\_N\_c≈ìurs}}
\]

Un **speedup id√©al** serait lin√©aire, c'est-√†-dire un gain proportionnel au nombre de c≈ìurs.

##### **Sc√©narios de test pour la scalabilit√© forte :**

| Nombre de processeurs | Nombre total de points | Points par processeur |
|-----------------------|------------------------|-----------------------|
| 1                     | 1,000,000              | 1,000,000             |
| 2                     | 1,000,000              | 500,000               |
| 4                     | 1,000,000              | 250,000               |
| 8                     | 1,000,000              | 125,000               |
| 16                    | 1,000,000              | 62,500                |
| 1                     | 10,000,000             | 10,000,000            |
| 2                     | 10,000,000             | 5,000,000             |
| 4                     | 10,000,000             | 2,500,000             |
| 8                     | 10,000,000             | 1,250,000             |
| 16                    | 10,000,000             | 625,000               |
| 1                     | 100,000,000            | 100,000,000           |
| 2                     | 100,000,000            | 50,000,000            |
| 4                     | 100,000,000            | 25,000,000            |
| 8                     | 100,000,000            | 12,500,000            |
| 16                    | 100,000,000            | 6,250,000             |

Dans ce cas, on garde le nombre total de points constant et on varie le nombre de processeurs pour observer la r√©duction du temps d'ex√©cution. Le **speedup** est mesur√© pour chaque configuration.

##### 2. **Scalabilit√© Faible**

La **scalabilit√© faible** √©value la capacit√© d'un programme √† maintenir un temps d'ex√©cution constant lorsque le nombre de c≈ìurs et la charge de travail totale augmentent proportionnellement. Cela simule un sc√©nario o√π chaque c≈ìur traite une part fixe de travail suppl√©mentaire.

Elle est √©galement mesur√©e √† l‚Äôaide du **speedup**, calcul√© de la m√™me mani√®re que pour la scalabilit√© forte, mais ici avec une charge croissante. Un **speedup id√©al** serait constant, ce qui se traduirait par une droite horizontale.

##### **Sc√©narios de test pour la scalabilit√© faible :**

| Nombre de processeurs | Nombre total de points | Points par processeur |
|-----------------------|------------------------|-----------------------|
| 1                     | 1,000,000              | 1,000,000             |
| 2                     | 2,000,000              | 1,000,000             |
| 4                     | 4,000,000              | 1,000,000             |
| 8                     | 8,000,000              | 1,000,000             |
| 16                    | 16,000,000             | 1,000,000             |
| 1                     | 10,000,000             | 10,000,000            |
| 2                     | 20,000,000             | 10,000,000            |
| 4                     | 40,000,000             | 10,000,000            |
| 8                     | 80,000,000             | 10,000,000            |
| 16                    | 160,000,000            | 10,000,000            |
| 1                     | 100,000,000            | 100,000,000           |
| 2                     | 200,000,000            | 100,000,000           |
| 4                     | 400,000,000            | 100,000,000           |
| 8                     | 800,000,000            | 100,000,000           |
| 16                    | 1,600,000,000          | 100,000,000           |

Dans ce cas, on augmente proportionnellement la charge de travail avec le nombre de c≈ìurs pour tester la capacit√© du programme √† maintenir un temps d'ex√©cution constant.

---
### C. R√©sultats de l'impl√©mentation Assignment102

Pour √©valuer la scalabilit√© de l'impl√©mentation **Assignment102**, le code a √©t√© modifi√© afin de permettre un contr√¥le pr√©cis du nombre de processeurs utilis√©s. Cela remplace l'utilisation dynamique de `Runtime.getRuntime().availableProcessors()` et nous permet de fixer le nombre de c≈ìurs pour chaque test via l'initialisation de la classe `PiMonteCarlo`.

#### 1. R√©sultats de la Scalabilit√© Forte

Les tests de **scalabilit√© forte** ont √©t√© effectu√©s en lan√ßant plusieurs simulations avec des configurations vari√©es de nombre de c≈ìurs et de points. Chaque test a √©t√© r√©p√©t√© cinq fois pour calculer la moyenne des r√©sultats.

| Nombre de c≈ìurs | Points lanc√©s | Points par c≈ìur | Temps d'ex√©cution (ms) | Approximation de œÄ | Erreur              |
|-----------------|---------------|-----------------|------------------------|-------------------|---------------------|
| 1               | 1,000,000     | 1,000,000       | 91.5                   | 3.1415024         | 3.02 √ó 10‚Åª‚Åµ        |
| 2               | 1,000,000     | 500,000         | 85.1                   | 3.1399276         | 5.37 √ó 10‚Åª‚Å¥        |
| 4               | 1,000,000     | 250,000         | 93.2                   | 3.14148           | 2.61 √ó 10‚Åª‚Åµ        |
| 8               | 1,000,000     | 125,000         | 124.8                  | 3.1420352         | 1.44 √ó 10‚Åª‚Å¥        |
| 16              | 1,000,000     | 62,500          | 128.6                  | 3.14108           | 1.62 √ó 10‚Åª‚Å¥        |
| 1               | 10,000,000    | 10,000,000      | 840.2                  | 3.14186892        | 8.45 √ó 10‚Åª‚Åµ        |
| 2               | 10,000,000    | 5,000,000       | 208.1                  | 3.14125864        | 1.08 √ó 10‚Åª‚Å¥        |
| 4               | 10,000,000    | 2,500,000       | 817.3                  | 3.14156388        | 1.14 √ó 10‚Åª‚Åµ        |
| 8               | 10,000,000    | 1,250,000       | 872.6                  | 3.1418712         | 8.55 √ó 10‚Åª‚Åµ        |
| 16              | 10,000,000    | 625,000         | 928.9                  | 3.14147052        | 4.08 √ó 10‚Åª‚Åµ        |
| 1               | 100,000,000   | 100,000,000     | 8,775.4                | 3.141591432       | 5.27 √ó 10‚Åª‚Å∑        |
| 2               | 100,000,000   | 50,000,000      | 8,362.7                | 3.141537294       | 1.81 √ó 10‚Åª‚Åµ        |
| 4               | 100,000,000   | 25,000,000      | 8,748.9                | 3.141649328       | 1.66 √ó 10‚Åª‚Åµ        |
| 8               | 100,000,000   | 12,500,000      | 8,723.1                | 3.141608412       | 5.59 √ó 10‚Åª‚Å∂        |
| 16              | 100,000,000   | 6,250,000       | 8,629.5                | 3.1416552         | 1.84 √ó 10‚Åª‚Åµ        |

En utilisant un programme Python pour calculer le **speedup** et g√©n√©rer un graphique, les r√©sultats de scalabilit√© forte montrent un **speedup** initial qui commence √† 1, mais diminue ensuite avant de se stabiliser sous 1 : 

---

![speedup assignment102](./images/scalabilite_forte_Assignment102_10000000.png)

---

##### Analyse des r√©sultats de scalabilit√© forte :

- **Surcharge de synchronisation** : L‚Äôutilisation de `AtomicInteger` pour g√©rer les ressources partag√©es introduit de la latence, ralentissant les threads au fur et √† mesure que leur nombre augmente.
- **Overhead li√© aux threads** : La gestion des threads devient co√ªteuse au-del√† d'un certain nombre de c≈ìurs, annulant les gains attendus de la parall√©lisation.
- **T√¢ches trop petites** : Lorsque la charge de travail par processeur devient trop petite, l‚Äôoverhead de la synchronisation d√©passe les b√©n√©fices de la parall√©lisation.

Ces facteurs expliquent la baisse du **speedup** et sugg√®rent que l'impl√©mentation n'est pas optimale au-del√† d'un certain nombre de c≈ìurs.

#### 2. R√©sultats de la Scalabilit√© Faible

Les tests de **scalabilit√© faible** ont √©galement √©t√© effectu√©s pour observer l'impact de l'augmentation du nombre de points, tout en augmentant proportionnellement le nombre de c≈ìurs.

| Nombre de c≈ìurs | Points lanc√©s | Points par c≈ìur | Temps d'ex√©cution (ms) | Approximation de œÄ | Erreur              |
|-----------------|---------------|-----------------|------------------------|-------------------|---------------------|
| 1               | 1,000,000     | 1,000,000       | 121.0                  | 3.1432558         | 5.32 √ó 10‚Åª‚Å¥        |
| 2               | 2,000,000     | 1,000,000       | 204.6                  | 3.141362          | 7.64 √ó 10‚Åª‚Åµ        |
| 4               | 4,000,000     | 1,000,000       | 355.2                  | 3.1415702         | 8.42 √ó 10‚Åª‚Å∂        |
| 8               | 8,000,000     | 1,000,000       | 772.9                  | 3.1415198         | 2.47 √ó 10‚Åª‚Åµ        |
| 16              | 16,000,000    | 1,000,000       | 1458.3                 | 3.1415634         | 8.56 √ó 10‚Åª‚Å∂        |
| 1               | 10,000,000    | 10,000,000      | 945.2                  | 3.14193692        | 1.11 √ó 10‚Åª‚Å¥        |
| 2               | 20,000,000    | 10,000,000      | 1824.8                 | 3.14175522        | 5.44 √ó 10‚Åª‚Åµ        |
| 4               | 40,000,000    | 10,000,000      | 3877.5                 | 3.14176904        | 5.64 √ó 10‚Åª‚Åµ        |
| 8               | 80,000,000    | 10,000,000      | 10110.3                | 3.14151792        | 2.46 √ó 10‚Åª‚Åµ        |
| 16              | 160,000,000   | 10,000,000      | 20318.1                | 3.141620478       | 9.45 √ó 10‚Åª‚Å∂        |
| 1               | 100,000,000   | 100,000,000     | 11315.4                | 3.141558261       | 1.18 √ó 10‚Åª‚Åµ        |

Les tests de scalabilit√© faible montrent une d√©gradation du **speedup**, particuli√®rement lorsqu‚Äôon double le nombre de points. Ce comportement est attendu, √©tant donn√© la limitation d'optimisation observ√©e lors des tests de scalabilit√© forte : 

---

![speedup assignment 102](./images/scalabilite_faible_Assignment102_10000000.png)

---

##### Analyse des r√©sultats de scalabilit√© faible :

Le **speedup** est loin d'√™tre lin√©aire et semble diminuer proportionnellement au nombre de points ajout√©s. Chaque fois que l‚Äôon double le nombre de points, le **speedup** est r√©duit de moiti√©.

Cette tendance n'est pas surprenante, √©tant donn√© les r√©sultats obtenus en scalabilit√© forte. Le **speedup** presque lin√©aire de cette derni√®re sugg√®re que l'ajout de c≈ìurs n‚Äôa pas d'impact significatif sur la performance du programme. Par cons√©quent, doubler le nombre de points m√®ne logiquement √† un temps d'ex√©cution deux fois plus long.

---

### D. R√©sultats Pi.Java

Aucune modification n'a √©t√© n√©cessaire pour la classe Pi.Java, car elle int√®gre d√©j√† une option permettant de limiter le nombre de workers.

#### Tableau des r√©sultats de scalabilit√© forte

Les tests de scalabilit√© forte ont √©t√© r√©p√©t√©s 5 fois pour calculer une moyenne. Voici les r√©sultats obtenus :

| Nombre de c≈ìurs | Points lanc√©s | Points par c≈ìur | Temps d'ex√©cution (ms) | Approximation de œÄ | Erreur       |
|-----------------|---------------|-----------------|------------------------|--------------------|--------------|
| 1               | 1,000,000     | 1,000,000       | 57.2                   | 3.1430894          | 4.79 √ó 10‚Åª‚Å¥ |
| 2               | 1,000,000     | 500,000         | 27.5                   | 3.141732           | 4.12 √ó 10‚Åª‚Åµ |
| 4               | 1,000,000     | 250,000         | 14.7                   | 3.140812           | 2.50 √ó 10‚Åª‚Å¥ |
| 8               | 1,000,000     | 125,000         | 11.9                   | 3.1418752          | 8.45 √ó 10‚Åª‚Åµ |
| 16              | 1,000,000     | 62,500          | 8.3                    | 3.1423106          | 2.30 √ó 10‚Åª‚Å¥ |
| 1               | 10,000,000    | 10,000,000      | 415.5                  | 3.14150231         | 2.65 √ó 10‚Åª‚Åµ |
| 2               | 10,000,000    | 5,000,000       | 218.3                  | 3.1416938          | 2.75 √ó 10‚Åª‚Åµ |
| 4               | 10,000,000    | 2,500,000       | 123.1                  | 3.14141985         | 5.60 √ó 10‚Åª‚Åµ |
| 8               | 10,000,000    | 1,250,000       | 68.9                   | 3.14098214         | 1.95 √ó 10‚Åª‚Å¥ |
| 16              | 10,000,000    | 625,000         | 59.4                   | 3.14135821         | 7.85 √ó 10‚Åª‚Åµ |
| 1               | 100,000,000   | 100,000,000     | 4328.7                 | 3.141610452        | 4.72 √ó 10‚Åª‚Å∂ |
| 2               | 100,000,000   | 50,000,000      | 2425.6                 | 3.141607899        | 3.95 √ó 10‚Åª‚Å∂ |
| 4               | 100,000,000   | 25,000,000      | 1252.8                 | 3.141658372        | 1.85 √ó 10‚Åª‚Åµ |
| 8               | 100,000,000   | 12,500,000      | 679.2                  | 3.141575841        | 4.63 √ó 10‚Åª‚Å∂ |
| 16              | 100,000,000   | 6,250,000       | 414.3                  | 3.141572018        | 5.38 √ó 10‚Åª‚Å∂ |

#### Courbe de scalabilit√© forte

---

![speedup pi](./images/scalabilite_forte_Pi.java_100000000.png)

---


La courbe de scalabilit√© forte suit une trajectoire presque lin√©aire sur une large plage de points, avant de l√©g√®rement d√©vier au-del√† de 8 c≈ìurs. Cependant, le speedup reste croissant, ce qui t√©moigne de l'efficacit√© de la parall√©lisation.

Avec 8 c≈ìurs physiques (2 c≈ìurs logiques chacun), l'impl√©mentation atteint une performance √©quivalente √† environ 11 c≈ìurs logiques, confirmant l'efficacit√© de la parall√©lisation dans ce code.

#### Tableau des r√©sultats de scalabilit√© faible

Les tests de scalabilit√© faible ont √©galement √©t√© r√©alis√©s, avec les r√©sultats suivants :

| Nombre de c≈ìurs | Points lanc√©s | Points par c≈ìur | Temps d'ex√©cution (ms) | Approximation de œÄ | Erreur       |
|-----------------|---------------|-----------------|------------------------|--------------------|--------------|
| 1               | 1,000,000     | 1,000,000       | 62.7                   | 3.1424102          | 2.58 √ó 10‚Åª‚Å¥ |
| 2               | 2,000,000     | 1,000,000       | 56.5                   | 3.1412984          | 9.72 √ó 10‚Åª‚Åµ |
| 4               | 4,000,000     | 1,000,000       | 65.1                   | 3.1418416          | 8.55 √ó 10‚Åª‚Åµ |
| 8               | 8,000,000     | 1,000,000       | 110.2                  | 3.1417208          | 4.58 √ó 10‚Åª‚Åµ |
| 16              | 16,000,000    | 1,000,000       | 157.1                  | 3.1414322          | 5.29 √ó 10‚Åª‚Åµ |
| 1               | 10,000,000    | 10,000,000      | 532.1                  | 3.14202256         | 1.43 √ó 10‚Åª‚Å¥ |
| 2               | 20,000,000    | 10,000,000      | 553.4                  | 3.14174292         | 5.08 √ó 10‚Åª‚Åµ |
| 4               | 40,000,000    | 10,000,000      | 535.8                  | 3.14163748         | 1.14 √ó 10‚Åª‚Åµ |
| 8               | 80,000,000    | 10,000,000      | 580.9                  | 3.14151422         | 2.62 √ó 10‚Åª‚Åµ |
| 16              | 160,000,000   | 10,000,000      | 778.0                  | 3.1415726          | 5.78 √ó 10‚Åª‚Å∂ |
| 1               | 100,000,000   | 100,000,000     | 4671.2                 | 3.141618472        | 6.12 √ó 10‚Åª‚Å∂ |
| 2               | 200,000,000   | 100,000,000     | 5112.8                 | 3.14152884         | 2.04 √ó 10‚Åª‚Åµ |
| 4               | 400,000,000   | 100,000,000     | 5263.4                 | 3.141575216        | 6.41 √ó 10‚Åª‚Å∂ |
| 8               | 800,000,000   | 100,000,000     | 7805.2                 | 3.141578102        | 5.16 √ó 10‚Åª‚Å∂ |
| 16              | 1,600,000,000 | 100,000,000     | 8723.8                 | 3.141582914        | 3.98 √ó 10‚Åª‚Å∂ |

#### Courbe de scalabilit√© faible

---

![speedup pi](./images/scalabilite_faible_Pi.java_100000000.png)

---


Le speedup d√©cro√Æt lentement √† mesure que le nombre de processeurs augmente. Bien que cette d√©croissance soit mod√©r√©e par rapport √† l'Assignment102, le speedup passe de 1 (avec un seul processeur) √† environ 0,75 avec 16 processeurs.

Cette baisse indique que le code Pi.java perd en efficacit√© parall√®le avec l'augmentation des ressources disponibles, mais cette perte reste contenue. Compar√© √† Assignment102, o√π la scalabilit√© chute bien plus rapidement, les r√©sultats de Pi.java restent globalement satisfaisants.

# V - Norme ISO : Efficiency et Effectiveness

La norme **ISO/IEC 25000 SQuaRE** (*Software Quality Requirements and Evaluation*) fournit un cadre structur√© pour d√©finir, √©valuer et am√©liorer la qualit√© des logiciels. Elle r√©pond √† un besoin de standardisation, garantissant une compr√©hension commune entre d√©veloppeurs, gestionnaires de projet et utilisateurs finaux. En uniformisant les crit√®res d‚Äô√©valuation, cette norme facilite la d√©tection et la correction des faiblesses des logiciels, contribuant ainsi √† leur am√©lioration continue. Son adoption renforce la confiance des utilisateurs et r√©duit les co√ªts de maintenance ainsi que les risques d‚Äô√©checs en production.

---

## Efficiency et Effectiveness dans la norme ISO/IEC 25000

Dans cette norme, **efficiency** (*efficience*) et **effectiveness** (*effectivit√©*) sont des notions fondamentales pour √©valuer la qualit√© d‚Äôun logiciel. Bien qu‚Äôelles puissent sembler similaires, leur application varie selon la perspective adopt√©e :

- **Product Quality (Qualit√© du produit)** : du point de vue technique.
- **Quality in Use (Qualit√© en utilisation)** : du point de vue de l‚Äôutilisateur final.

---

## **Product Quality (Qualit√© du produit)**

- **Cible** : D√©veloppeurs et √©quipes techniques.
- **Objectif** : Analyser les caract√©ristiques internes du logiciel (code, architecture, performance).
- **Exemple** : Un logiciel bien structur√©, respectant les principes de performance, maintenabilit√© et extensibilit√©, sera plus facile √† d√©ployer, maintenir et faire √©voluer.

### **D√©finitions dans le contexte de Product Quality :**
- **Efficiency** (*Efficience*) : Performance du logiciel en fonction des ressources utilis√©es (*temps, m√©moire, processeur, etc.*) pour accomplir une t√¢che dans des conditions sp√©cifi√©es.
- **Effectiveness** (*Effectivit√©*) : Capacit√© du logiciel √† atteindre ses objectifs fonctionnels en fournissant des r√©sultats corrects et pr√©cis.

> **Exemple** : Un logiciel de calcul est "effectif" s'il produit des r√©sultats exacts, ind√©pendamment de l‚Äôoptimisation des ressources utilis√©es.

### **Performance Efficiency**
L‚Äôefficience du produit est √©valu√©e par :
- **Le temps d‚Äôex√©cution** : rapidit√© avec laquelle le logiciel accomplit ses t√¢ches.
- **La consommation m√©moire** : quantit√© de m√©moire utilis√©e pendant les op√©rations.

---

## **Quality in Use (Qualit√© en utilisation)**

- **Cible** : Utilisateurs finaux et clients.
- **Objectif** : √âvaluer si le logiciel permet aux utilisateurs d‚Äôatteindre leurs objectifs dans des sc√©narios d‚Äôutilisation r√©els.
- **Exemple** : Une application mobile qui permet aux utilisateurs de r√©server un billet d‚Äôavion rapidement, sans frustration ni erreur, sera per√ßue comme **efficace** (*effective*) et **efficiente** (*efficient*).

### **D√©finitions dans le contexte de Quality in Use :**
- **Efficiency** (*Efficience*) : Quantit√© de ressources (*temps, effort, nombre d‚Äôinteractions, etc.*) n√©cessaires pour accomplir une t√¢che.
- **Effectiveness** (*Effectivit√©*) : Capacit√© de l‚Äôutilisateur √† atteindre ses objectifs sans erreur ni difficult√©.

> **Exemple** : Une application de r√©servation d‚Äôavion est "effective" si l‚Äôutilisateur peut finaliser sa r√©servation avec succ√®s.

---

## **Comparaison : Product Quality vs Quality in Use**

| **Crit√®re**        | **Product Quality** (Qualit√© du produit) | **Quality in Use** (Qualit√© en utilisation) |
|--------------------|--------------------------------|--------------------------------|
| **Cible**        | D√©veloppeurs & √©quipes techniques | Utilisateurs finaux & clients |
| **Perspective**  | Performance et architecture interne | Exp√©rience utilisateur |
| **Efficiency**   | Optimisation des ressources syst√®me | Optimisation des ressources utilisateur |
| **Effectiveness** | Pr√©cision des r√©sultats | Succ√®s dans l‚Äôatteinte des objectifs |

### **Diff√©rences cl√©s :**
- **Product Quality** mesure la performance interne du logiciel en termes d‚Äôutilisation des ressources techniques.
- **Quality in Use** √©value l‚Äôimpact sur l‚Äôutilisateur en analysant son interaction avec le produit et sa capacit√© √† atteindre ses objectifs.

Bien que ces deux notions √©valuent des qualit√©s similaires, elles diff√®rent par leur perspective :
- **Product Quality** concerne l‚Äôoptimisation technique du logiciel.
- **Quality in Use** mesure l‚Äôefficacit√© per√ßue par l‚Äôutilisateur final.

Ces deux approches sont compl√©mentaires et permettent une √©valuation compl√®te de la qualit√© logicielle, tant du point de vue technique que du point de vue utilisateur.
