# Introduction

Ce rapport explore l'utilisation de la m√©thode de Monte Carlo (MC) pour calculer œÄ en exploitant le parall√©lisme sur des architectures √† m√©moire partag√©e et distribu√©e. Apr√®s une pr√©sentation de l‚Äôalgorithme s√©quentiel, nous √©tudions des variantes parall√®les (it√©ration parall√®le, ma√Ætre-esclave) et analysons deux impl√©mentations Java.

Enfin, nous √©tendons l‚Äô√©tude aux environnements √† m√©moire distribu√©e et au parall√©lisme sur plusieurs machines, en √©valuant les performances des diff√©rentes approches. Ce travail vise √† fournir une vue synth√©tique et claire des strat√©gies et r√©sultats obtenus.

Ce rapport a √©t√© en partie r√©dig√© par ChatGPT, dans le but de le simplifier et de le rendre le plus clair et concis possible.

# I. Monte Carlo pour calculer œÄ

La m√©thode de Monte Carlo repose sur une estimation probabiliste pour approximer œÄ √† partir de tirages al√©atoires.

Soit AD l‚Äôaire d‚Äôun  de disque de rayon ( r = 1 ) :
$$
A_{\text{D}} = \frac{\pi r^2}{4} = \frac{\pi}{4}
$$

Le disque est inscrit dans un carr√© de c√¥t√© ( r = 1 ), dont l‚Äôaire est :
$$
A_c = r^2 = 1
$$

On consid√®re un point ( X_p (x_p, y_p) ) g√©n√©r√© al√©atoirement dans ce carr√©, o√π ( x_p ) et ( y_p ) suivent la loi uniforme ( U(0, 1) ).

La probabilit√© que ( X_p ) appartienne au  de disque est donn√©e par :
$$
P = \frac{A_{\text{D}}}{A_c} = \frac{\pi}{4}
$$

-------------------- 

![Monte Carlo](./images/figure_monte_carlo-1.png)

--------------------

Pour estimer cette probabilit√©, on effectue n tot tirages al√©atoires. Soit n cible le nombre de points qui satisfont la condition x p 2 + y p 2 ‚â§ 1 , c‚Äôest-√†-dire les points situ√©s dans le quart de disque.

Si n tot est suffisamment grand, par la loi des grands nombres, la fr√©quence observ√©e n cible / n tot converge vers la probabilit√© P , soit :
P = n cible n tot ‚âà œÄ 4

On peut ainsi en d√©duire une approximation de œÄ :
œÄ ‚âà 4 ‚ãÖ n cible n tot

Ainsi, plus n tot augmente, plus l'estimation de œÄ se pr√©cise.

## II. Algorithme et parall√©lisation

### A. It√©ration parall√®le

L‚Äôalgorithme de Monte Carlo peut √™tre parall√©lis√© en distribuant les tirages sur plusieurs t√¢ches ind√©pendantes. Voici l'algorithme s√©quentiel de base :

#### Algorithme de base

```c
n_cible = 0;
for (p = 0; n_tot > 0; n_tot--) {
    x_p = rand();  // G√©n√©rer un nombre al√©atoire entre ]0,1[
    y_p = rand();
    if ((x_p * x_p + y_p * y_p) < 1) {
        n_cible++;
    }
}
pi = 4 * n_cible / n_tot;
```

Dans cette version, tout est ex√©cut√© s√©quentiellement. Pour parall√©liser ce code, il faut identifier les t√¢ches et leurs d√©pendances.
Identification des t√¢ches

    T√¢che principale (T0) : Tirer et compter les n_tot points.

    T√¢che secondaire (T1) : Calculer œÄœÄ apr√®s la collecte de n_cible :

D√©pendances et parall√©lisme

    D√©pendances :
        T1 d√©pend de T0 : œÄœÄ ne peut √™tre calcul√© qu‚Äôapr√®s avoir obtenu n_cible.
        T0p2 d√©pend de T0p1 : Un point doit √™tre g√©n√©r√© avant sa v√©rification.

    Ind√©pendances parall√®les :
        Les g√©n√©rations de points (T0p1) peuvent √™tre effectu√©es simultan√©ment, chaque tirage √©tant ind√©pendant.
        Les v√©rifications (T0p2) peuvent aussi √™tre parall√©lis√©es, mais n√©cessitent une gestion s√©curis√©e de n_cible pour √©viter des conflits d‚Äôacc√®s.

Algorithme parall√®le

L‚Äôalgorithme parall√®le repose sur une fonction d√©di√©e TirerPoint() pour g√©n√©rer et √©valuer les points.

TirerPoint() est ind√©pendante et permet donc d'√©xecuter chaque tirage sur plusieurs threads sans d√©pendance entre eux.

# B. Paradigme Master/Worker

Le mod√®le **Master/Worker** repose sur une division du travail en unit√©s ind√©pendantes, chacune trait√©e par un **Worker** (processus ou thread). Chaque Worker effectue un certain nombre de tirages al√©atoires, et un processus principal, appel√© **Master**, agr√®ge les r√©sultats pour calculer œÄ.

## 1. Principe du mod√®le

- Le travail total (`n_tot` tirages) est r√©parti √©quitablement entre `n_workers`.
- Chaque Worker ex√©cute une fonction ind√©pendante pour traiter sa part des tirages.
- Le Master collecte les r√©sultats des Workers et calcule l'estimation finale de œÄ.

Ce mod√®le permet d'optimiser l'utilisation des ressources tout en minimisant les conflits d'acc√®s m√©moire.

---

## 2. Algorithme Master/Worker

### **Fonctions principales**

```cpp
function TirerPoint() {
    x_p = rand();  // G√©n√©rer un nombre al√©atoire dans ]0,1[
    y_p = rand();
    return ((x_p * x_p + y_p * y_p) < 1);
}

function MCWorker(n_charge) {
    n_cible_partiel = 0;
    for (p = 0; n_charge > 0; n_charge--) {
        if (TirerPoint()) {
            n_cible_partiel += 1;
        }
    }
    return n_cible_partiel;
}
```
- TirerPoint() : g√©n√®re un point al√©atoire et v√©rifie s'il appartient au quart de disque.
- MCWorker(n_charge) : effectue n_charge tirages et compte ceux appartenant au quart de disque.

```cpp
n_charge = n_tot / n_workers;
ncibles = [NULL * n_workers];

parallel for (worker = 0; worker < n_workers; worker++) {
    ncibles[worker] = MCWorker(n_charge);
}

n_cible = sum(ncibles);
pi = 4 * n_cible / n_tot;
```

- Chaque Worker traite n_charge = n_tot / n_workers points.
- Le tableau ncibles stocke les r√©sultats de chaque Worker.
- Une fois le calcul termin√©, le Master additionne les valeurs et estime œÄ.

## 3. Gestion des ressources et parall√©lisation
√âlimination des conflits d'acc√®s

Contrairement √† l‚Äôit√©ration parall√®le, o√π plusieurs threads modifiaient une variable partag√©e (n_cible), ici :

    Chaque Worker poss√®de son propre compteur local (n_cible_partiel), √©vitant ainsi les conflits.
    Le seul √©l√©ment partag√© est le tableau ncibles, mais chaque Worker √©crit dans une case distincte.

Optimisation et scalabilit√©

    R√©duction des conflits : aucun verrouillage n√©cessaire, am√©liorant l‚Äôefficacit√©.
    R√©partition √©quilibr√©e : chaque Worker traite une charge √©quivalente, ce qui optimise l'utilisation des ressources.
    Adaptabilit√© aux architectures distribu√©es : le mod√®le fonctionne aussi bien sur des architectures multi-c≈ìurs que sur des syst√®mes distribu√©s.


# 4. Avantages du mod√®le Master/Worker

| **Crit√®re**         | **Avantage** |
|---------------------|-------------|
| **Synchronisation** | R√©duction des conflits gr√¢ce √† des compteurs locaux |
| **Efficacit√©**      | Ex√©cution parall√®le optimis√©e sans acc√®s concurrent √† une variable critique |
| **Scalabilit√©**     | Supporte un grand nombre de Workers sans perte de performance |
| **Modularit√©**      | Adaptable aux architectures distribu√©es, avec des Workers sur diff√©rentes machines |

# III. Mise en ≈ìuvre sur Machine

Nous allons maintenant examiner deux impl√©mentations pratiques de la m√©thode de Monte Carlo pour le calcul de œÄ. L‚Äôobjectif est d‚Äôanalyser leur structure et leur approche de parall√©lisation :

- Identifier le mod√®le de programmation parall√®le utilis√© dans chaque code ainsi que le paradigme suivi (it√©ration parall√®le ou Master/Worker).
- V√©rifier si ces impl√©mentations correspondent aux algorithmes propos√©s en **partie II**.

Dans la **partie IV**, nous effectuerons une analyse d√©taill√©e de chaque code en √©valuant leur **scalabilit√© forte et faible**.

---

## A. Analyse de *Assignment102*

L‚Äôimpl√©mentation *Assignment102* utilise l‚ÄôAPI **Concurrent** pour parall√©liser les calculs n√©cessaires √† l‚Äôestimation de œÄ avec la m√©thode de Monte Carlo. Voici les principaux √©l√©ments analys√©s :

### 1. Structure et API utilis√©e

#### Gestion des threads :
- Le code utilise `ExecutorService` avec un **pool de threads adaptatif** (`newWorkStealingPool`), exploitant efficacement les c≈ìurs disponibles sur la machine.
- Chaque tirage (g√©n√©ration d‚Äôun point al√©atoire) est ex√©cut√© dans une t√¢che ind√©pendante via `Runnable`.

#### Synchronisation avec AtomicInteger :
- La variable partag√©e `nAtomSuccess`, qui compte le nombre de points dans le quart de disque, est prot√©g√©e par un compteur atomique (`AtomicInteger`) pour √©viter les **conflits d‚Äôacc√®s** entre threads.

---

### 2. Mod√®le de programmation parall√®le et paradigme

| **Aspect**        | **D√©tails** |
|------------------|------------|
| **Mod√®le utilis√©** | **It√©ration parall√®le** : chaque tirage correspond √† une t√¢che ind√©pendante soumise au pool de threads. |
| **Paradigme**     | Suit le mod√®le d‚Äôit√©ration parall√®le d√©fini en **partie II.A**. Chaque t√¢che effectue un tirage de mani√®re ind√©pendante, sans d√©pendances entre elles. |

---

### 3. Lien avec notre pseudo-code

L‚Äôimpl√©mentation correspond globalement au pseudo-code d‚Äô**it√©ration parall√®le**, avec les adaptations suivantes :

- Le compteur `n_cible` est remplac√© par un **AtomicInteger** pour g√©rer les **sections critiques**.
- Le d√©couplage des threads est **g√©r√© par l‚ÄôAPI `ExecutorService`**.

---


## B. Analyse de *Pi.java*

L‚Äôimpl√©mentation *Pi.java* repose sur l‚Äôutilisation des **Futures et Callables** pour parall√©liser le calcul de œÄ √† l‚Äôaide de la m√©thode de Monte Carlo.

### 1. Qu‚Äôest-ce qu‚Äôun Future ?  
Un **Future** est un conteneur pour un r√©sultat **calcul√© de mani√®re asynchrone**. Il permet de :

- **Soumettre une t√¢che** : lorsqu‚Äôun thread ex√©cute une t√¢che, son r√©sultat est **stock√©** dans un Future.
- **R√©cup√©rer le r√©sultat** : l‚Äôappel √† `.get()` r√©cup√®re la valeur, mais **bloque** tant que le calcul n‚Äôest pas termin√©.  
  ‚Üí Cela introduit une **barri√®re implicite** qui synchronise les r√©sultats des diff√©rents threads.
- **V√©rifier l‚Äô√©tat d‚Äôex√©cution** : un `Future` peut indiquer si une t√¢che est **termin√©e ou a √©chou√©**.

**Pourquoi l‚Äôutiliser ici ?**  
Les **Futures** garantissent que chaque r√©sultat partiel est **pr√™t avant l‚Äôagr√©gation**, permettant une synchronisation **optimale** entre les threads.

---

![Diagramme de Classe](./images/DiagrammeClasse.png)

---

### 2. Mod√®le de programmation parall√®le et paradigme

| **Aspect**        | **D√©tails** |
|------------------|------------|
| **Mod√®le utilis√©** | **Master/Worker** : le Master cr√©e des Workers pour effectuer les calculs et regroupe leurs r√©sultats √† l‚Äôaide des **Futures**. |
| **Paradigme**     | Bas√© sur des **t√¢ches**, avec une gestion explicite via **Callables**. |

---

### 3. Structure et API utilis√©e

#### üîπ Parall√©lisation avec des Callables :
- Chaque **Worker** est un `Callable<Long>` qui ex√©cute un sous-ensemble du calcul total.
- Il **compte les points dans le quart de disque** pour un certain nombre d‚Äôit√©rations.
- Ces Callables sont ex√©cut√©s par un **pool de threads fixe** (`FixedThreadPool`).

#### üîπ Gestion des r√©sultats avec des Futures :
- Lorsqu‚Äôun `Callable` est soumis au pool de threads, il retourne un **Future<Long>**.
- L‚Äôappel √† `.get()` bloque le thread principal jusqu‚Äô√† ce que **tous les r√©sultats** soient pr√™ts.
- Une fois collect√©s, ces r√©sultats sont **agr√©g√©s** pour calculer œÄ.

---

### 4. Lien avec notre pseudo-code

L'algorithme suit fid√®lement le mod√®le **Master/Worker** d√©crit en **partie II.B** :

| **√âl√©ment**       | **Correspondance dans le code** |
|------------------|--------------------------------|
| **Master**       | G√®re la r√©partition des t√¢ches et l‚Äôagr√©gation des r√©sultats. |
| **Workers**      | Ex√©cut√©s via des `Callables`, chaque Worker applique la m√©thode `MCWorker()`. |
| **Division**     | Le Master r√©partit **√©quitablement** les tirages (`n_charge`) entre les Workers. |

---

### 5. Comparaison avec *Assignment102*

| **Crit√®re**           | **Pi.java (Master/Worker)** | **Assignment102 (It√©ration parall√®le)** |
|----------------------|---------------------------|----------------------------------|
| **Isolation des calculs** | Chaque Worker g√®re ses propres donn√©es. | Acc√®s concurrent √† une variable atomique. |
| **Synchronisation** | R√©duite √† l‚Äôagr√©gation finale (moins co√ªteux). | Synchronisation fr√©quente via `AtomicInteger`. |
| **Gestion des threads** | Callables et `FixedThreadPool`. | `ExecutorService` avec `Runnable`. |
| **Performance attendue** | Plus efficace pour un grand nombre de threads. | Risque de **goulots d‚Äô√©tranglement** d√ª aux acc√®s atomiques. |

---

üîé **Conclusion g√©n√©rale**  
L‚Äôimpl√©mentation *Pi.java* est **plus efficace** qu‚Äô*Assignment102* car elle :
1. **R√©duit la synchronisation co√ªteuse** (seule l‚Äôagr√©gation des r√©sultats est bloquante).
2. **Minimise les acc√®s concurrents** gr√¢ce √† l‚Äô**isolation des Workers**.
3. **Optimise la gestion des threads** via les **Futures**, permettant une meilleure scalabilit√©.

On peut donc **s‚Äôattendre √† de meilleures performances**, **surtout sur des machines multic≈ìurs** et avec **un grand nombre de points et de threads**. 

# IV. √âvaluations et tests de performances

La partie suivante contient des √©l√©ments d'un autre rapport que j'√©cris en parall√®le de celui-ci, qui concerne le module de Qualit√© de D√©veloppement en troisi√®me ann√©e de BUT informatique.

L'ordinateur qui a r√©alis√© ces calculs poss√®de les sp√©cifications suivantes :

- **Processeur** : 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz
- **8 c≈ìurs physiques**
- **16 c≈ìurs logiques**

Notez que les r√©sultats des tests qui suivent ne seront pas les m√™mes selon l'architecture mat√©rielle sur lesquels ils ont √©t√© effectu√©s.

### A. Programme de calcul de performance

Le script **PerformanceTester.java** teste diff√©rentes impl√©mentations de Monte Carlo pour calculer œÄ en s'appuyant sur une interface standardis√©e **MonteCarloImplementation**. Chaque impl√©mentation doit fournir deux m√©thodes : `execute(int totalPoints, int numCores)` et `getName()`.

Le programme r√©cup√®re les donn√©es de tests (nombre de c≈ìurs, de points, et r√©p√©titions) depuis un fichier CSV. Il ex√©cute ensuite les tests sur chaque impl√©mentation puis enregistre les r√©sultats (temps d'ex√©cution, approximation de œÄ, erreur relative) dans un fichier `resultats.csv`.

L'outil est modulaire, ce qui permet d'ajouter facilement de nouvelles versions de Monte Carlo √† tester. Il automatise √©galement l'√©valuation des performances pour diff√©rents sc√©narios et configurations.

---

### B. Tests de Scalabilit√©

Les tests de scalabilit√© servent √† √©valuer la performance des diff√©rentes impl√©mentations en fonction du nombre de c≈ìurs utilis√©s, selon deux approches : **scalabilit√© forte** et **scalabilit√© faible**.

##### 1. **Scalabilit√© Forte**

La **scalabilit√© forte** mesure la capacit√© d'un programme √† r√©duire son temps d'ex√©cution lorsque le nombre de c≈ìurs augmente, tout en maintenant la charge de travail totale constante. Elle √©value comment le programme exploite efficacement les ressources suppl√©mentaires.

On la mesure √† l‚Äôaide du **speedup**, d√©fini comme :

\[
\text{Speedup} = \frac{\text{Temps\_1\_c≈ìur}}{\text{Temps\_N\_c≈ìurs}}
\]

Un **speedup id√©al** serait lin√©aire, c'est-√†-dire un gain proportionnel au nombre de c≈ìurs.

##### **Sc√©narios de test pour la scalabilit√© forte :**

| Nombre de processeurs | Nombre total de points | Points par processeur |
|-----------------------|------------------------|-----------------------|
| 1                     | 1,000,000              | 1,000,000             |
| 2                     | 1,000,000              | 500,000               |
| 4                     | 1,000,000              | 250,000               |
| 8                     | 1,000,000              | 125,000               |
| 16                    | 1,000,000              | 62,500                |
| 1                     | 10,000,000             | 10,000,000            |
| 2                     | 10,000,000             | 5,000,000             |
| 4                     | 10,000,000             | 2,500,000             |
| 8                     | 10,000,000             | 1,250,000             |
| 16                    | 10,000,000             | 625,000               |
| 1                     | 100,000,000            | 100,000,000           |
| 2                     | 100,000,000            | 50,000,000            |
| 4                     | 100,000,000            | 25,000,000            |
| 8                     | 100,000,000            | 12,500,000            |
| 16                    | 100,000,000            | 6,250,000             |

Dans ce cas, on garde le nombre total de points constant et on varie le nombre de processeurs pour observer la r√©duction du temps d'ex√©cution. Le **speedup** est mesur√© pour chaque configuration.

##### 2. **Scalabilit√© Faible**

La **scalabilit√© faible** √©value la capacit√© d'un programme √† maintenir un temps d'ex√©cution constant lorsque le nombre de c≈ìurs et la charge de travail totale augmentent proportionnellement. Cela simule un sc√©nario o√π chaque c≈ìur traite une part fixe de travail suppl√©mentaire.

Elle est √©galement mesur√©e √† l‚Äôaide du **speedup**, calcul√© de la m√™me mani√®re que pour la scalabilit√© forte, mais ici avec une charge croissante. Un **speedup id√©al** serait constant, ce qui se traduirait par une droite horizontale.

##### **Sc√©narios de test pour la scalabilit√© faible :**

| Nombre de processeurs | Nombre total de points | Points par processeur |
|-----------------------|------------------------|-----------------------|
| 1                     | 1,000,000              | 1,000,000             |
| 2                     | 2,000,000              | 1,000,000             |
| 4                     | 4,000,000              | 1,000,000             |
| 8                     | 8,000,000              | 1,000,000             |
| 16                    | 16,000,000             | 1,000,000             |
| 1                     | 10,000,000             | 10,000,000            |
| 2                     | 20,000,000             | 10,000,000            |
| 4                     | 40,000,000             | 10,000,000            |
| 8                     | 80,000,000             | 10,000,000            |
| 16                    | 160,000,000            | 10,000,000            |
| 1                     | 100,000,000            | 100,000,000           |
| 2                     | 200,000,000            | 100,000,000           |
| 4                     | 400,000,000            | 100,000,000           |
| 8                     | 800,000,000            | 100,000,000           |
| 16                    | 1,600,000,000          | 100,000,000           |

Dans ce cas, on augmente proportionnellement la charge de travail avec le nombre de c≈ìurs pour tester la capacit√© du programme √† maintenir un temps d'ex√©cution constant.

---
### C. R√©sultats de l'impl√©mentation Assignment102

Pour √©valuer la scalabilit√© de l'impl√©mentation **Assignment102**, le code a √©t√© modifi√© afin de permettre un contr√¥le pr√©cis du nombre de processeurs utilis√©s. Cela remplace l'utilisation dynamique de `Runtime.getRuntime().availableProcessors()` et nous permet de fixer le nombre de c≈ìurs pour chaque test via l'initialisation de la classe `PiMonteCarlo`.

#### 1. R√©sultats de la Scalabilit√© Forte

Les tests de **scalabilit√© forte** ont √©t√© effectu√©s en lan√ßant plusieurs simulations avec des configurations vari√©es de nombre de c≈ìurs et de points. Chaque test a √©t√© r√©p√©t√© cinq fois pour calculer la moyenne des r√©sultats.

| Nombre de c≈ìurs | Points lanc√©s | Points par c≈ìur | Temps d'ex√©cution (ms) | Approximation de œÄ | Erreur              |
|-----------------|---------------|-----------------|------------------------|-------------------|---------------------|
| 1               | 1,000,000     | 1,000,000       | 92.0                   | 3.1414976         | 3.03 √ó 10‚Åª‚Åµ        |
| 2               | 1,000,000     | 500,000         | 84.2                   | 3.1398992         | 5.39 √ó 10‚Åª‚Å¥        |
| 4               | 1,000,000     | 250,000         | 92.6                   | 3.14152           | 2.31 √ó 10‚Åª‚Åµ        |
| 8               | 1,000,000     | 125,000         | 126.2                  | 3.1420408         | 1.43 √ó 10‚Åª‚Å¥        |
| 16              | 1,000,000     | 62,500          | 127.2                  | 3.1411            | 1.57 √ó 10‚Åª‚Å¥        |
| 1               | 10,000,000    | 10,000,000      | 836.8                  | 3.14186112        | 8.55 √ó 10‚Åª‚Åµ        |
| 2               | 10,000,000    | 5,000,000       | 206.4                  | 3.14125008        | 1.09 √ó 10‚Åª‚Å¥        |
| 4               | 10,000,000    | 2,500,000       | 819.8                  | 3.14155624        | 1.16 √ó 10‚Åª‚Åµ        |
| 8               | 10,000,000    | 1,250,000       | 869.0                  | 3.1418644         | 8.65 √ó 10‚Åª‚Åµ        |
| 16              | 10,000,000    | 625,000         | 925.4                  | 3.14146304        | 4.13 √ó 10‚Åª‚Åµ        |
| 1               | 100,000,000   | 100,000,000     | 8,788.0                | 3.141590984       | 5.31 √ó 10‚Åª‚Å∑        |
| 2               | 100,000,000   | 50,000,000      | 8,358.2                | 3.141534816       | 1.84 √ó 10‚Åª‚Åµ        |
| 4               | 100,000,000   | 25,000,000      | 8,743.0                | 3.141645944       | 1.70 √ó 10‚Åª‚Åµ        |
| 8               | 100,000,000   | 12,500,000      | 8,728.4                | 3.141609744       | 5.44 √ó 10‚Åª‚Å∂        |
| 16              | 100,000,000   | 6,250,000       | 8,625.2                | 3.1416518         | 1.88 √ó 10‚Åª‚Åµ        |

En utilisant un programme Python pour calculer le **speedup** et g√©n√©rer un graphique, les r√©sultats de scalabilit√© forte montrent un **speedup** initial qui commence √† 1, mais diminue ensuite avant de se stabiliser sous 1 : 

---

![speedup assignment102](./images/scalabilite_forte_Assignment102_10000000.png)

---

##### Analyse des r√©sultats de scalabilit√© forte :

- **Surcharge de synchronisation** : L‚Äôutilisation de `AtomicInteger` pour g√©rer les ressources partag√©es introduit de la latence, ralentissant les threads au fur et √† mesure que leur nombre augmente.
- **Overhead li√© aux threads** : La gestion des threads devient co√ªteuse au-del√† d'un certain nombre de c≈ìurs, annulant les gains attendus de la parall√©lisation.
- **T√¢ches trop petites** : Lorsque la charge de travail par processeur devient trop petite, l‚Äôoverhead de la synchronisation d√©passe les b√©n√©fices de la parall√©lisation.

Ces facteurs expliquent la baisse du **speedup** et sugg√®rent que l'impl√©mentation n'est pas optimale au-del√† d'un certain nombre de c≈ìurs.

#### 2. R√©sultats de la Scalabilit√© Faible

Les tests de **scalabilit√© faible** ont √©galement √©t√© effectu√©s pour observer l'impact de l'augmentation du nombre de points, tout en augmentant proportionnellement le nombre de c≈ìurs.

| Nombre de c≈ìurs | Points lanc√©s | Points par c≈ìur | Temps d'ex√©cution (ms) | Approximation de œÄ | Erreur              |
|-----------------|---------------|-----------------|------------------------|-------------------|---------------------|
| 1               | 1,000,000     | 1,000,000       | 120.2                  | 3.1432696         | 5.34 √ó 10‚Åª‚Å¥        |
| 2               | 2,000,000     | 1,000,000       | 205.0                  | 3.141348          | 7.79 √ó 10‚Åª‚Åµ        |
| 4               | 4,000,000     | 1,000,000       | 354.6                  | 3.1415638         | 9.18 √ó 10‚Åª‚Å∂        |
| 8               | 8,000,000     | 1,000,000       | 773.6                  | 3.1415113         | 2.59 √ó 10‚Åª‚Åµ        |
| 16              | 16,000,000    | 1,000,000       | 1456.8                 | 3.1415671         | 8.13 √ó 10‚Åª‚Å∂        |
| 1               | 10,000,000    | 10,000,000      | 943.8                  | 3.14194448        | 1.12 √ó 10‚Åª‚Å¥        |
| 2               | 20,000,000    | 10,000,000      | 1826.0                 | 3.14176164        | 5.38 √ó 10‚Åª‚Åµ        |
| 4               | 40,000,000    | 10,000,000      | 3875.0                 | 3.14177182        | 5.70 √ó 10‚Åª‚Åµ        |
| 8               | 80,000,000    | 10,000,000      | 10112.6                | 3.14151361        | 2.52 √ó 10‚Åª‚Åµ        |
| 16              | 160,000,000   | 10,000,000      | 20315.6                | 3.141623355       | 9.77 √ó 10‚Åª‚Å∂        |
| 1               | 100,000,000   | 100,000,000     | 11313.8                | 3.141555048       | 1.20 √ó 10‚Åª‚Åµ        |

Les tests de scalabilit√© faible montrent une d√©gradation du **speedup**, particuli√®rement lorsqu‚Äôon double le nombre de points. Ce comportement est attendu, √©tant donn√© la limitation d'optimisation observ√©e lors des tests de scalabilit√© forte : 

---

![speedup assignment 102](./images/scalabilite_faible_Assignment102_10000000.png)

---

##### Analyse des r√©sultats de scalabilit√© faible :

Le **speedup** est loin d'√™tre lin√©aire et semble diminuer proportionnellement au nombre de points ajout√©s. Chaque fois que l‚Äôon double le nombre de points, le **speedup** est r√©duit de moiti√©.

Cette tendance n'est pas surprenante, √©tant donn√© les r√©sultats obtenus en scalabilit√© forte. Le **speedup** presque lin√©aire de cette derni√®re sugg√®re que l'ajout de c≈ìurs n‚Äôa pas d'impact significatif sur la performance du programme. Par cons√©quent, doubler le nombre de points m√®ne logiquement √† un temps d'ex√©cution deux fois plus long.

---

### D. R√©sultats Pi.Java

Aucune modification n'a √©t√© n√©cessaire pour la classe Pi.Java, car elle int√®gre d√©j√† une option permettant de limiter le nombre de workers.

#### Tableau des r√©sultats de scalabilit√© forte

Les tests de scalabilit√© forte ont √©t√© r√©p√©t√©s 5 fois pour calculer une moyenne. Voici les r√©sultats obtenus :

| Nombre de c≈ìurs | Points lanc√©s | Points par c≈ìur | Temps d'ex√©cution (ms) | Approximation de œÄ | Erreur       |
|-----------------|---------------|-----------------|------------------------|--------------------|--------------|
| 1               | 1,000,000     | 1,000,000       | 56.8                   | 3.1431048          | 4.81 √ó 10‚Åª‚Å¥ |
| 2               | 1,000,000     | 500,000         | 27.0                   | 3.141724           | 4.18 √ó 10‚Åª‚Åµ |
| 4               | 1,000,000     | 250,000         | 14.4                   | 3.1408             | 2.52 √ó 10‚Åª‚Å¥ |
| 8               | 1,000,000     | 125,000         | 11.6                   | 3.1418616          | 8.56 √ó 10‚Åª‚Åµ |
| 16              | 1,000,000     | 62,500          | 8.0                    | 3.1423224          | 2.32 √ó 10‚Åª‚Å¥ |
| 1               | 10,000,000    | 10,000,000      | 414.0                  | 3.14151048         | 2.62 √ó 10‚Åª‚Åµ |
| 2               | 10,000,000    | 5,000,000       | 217.8                  | 3.1416816          | 2.83 √ó 10‚Åª‚Åµ |
| 4               | 10,000,000    | 2,500,000       | 122.4                  | 3.14141312         | 5.71 √ó 10‚Åª‚Åµ |
| 8               | 10,000,000    | 1,250,000       | 68.4                   | 3.14099672         | 1.90 √ó 10‚Åª‚Å¥ |
| 16              | 10,000,000    | 625,000         | 59.0                   | 3.14134416         | 7.91 √ó 10‚Åª‚Åµ |
| 1               | 100,000,000   | 100,000,000     | 4335.0                 | 3.141607864        | 4.84 √ó 10‚Åª‚Å∂ |
| 2               | 100,000,000   | 50,000,000      | 2421.2                 | 3.141605256        | 4.01 √ó 10‚Åª‚Å∂ |
| 4               | 100,000,000   | 25,000,000      | 1250.4                 | 3.141652968        | 1.92 √ó 10‚Åª‚Åµ |
| 8               | 100,000,000   | 12,500,000      | 677.4                  | 3.141578304        | 4.57 √ó 10‚Åª‚Å∂ |
| 16              | 100,000,000   | 6,250,000       | 412.0                  | 3.141576256        | 5.22 √ó 10‚Åª‚Å∂ |

#### Courbe de scalabilit√© forte

---

![speedup pi](./images/scalabilite_forte_Pi.java_100000000.png)

---


La courbe de scalabilit√© forte suit une trajectoire presque lin√©aire sur une large plage de points, avant de l√©g√®rement d√©vier au-del√† de 8 c≈ìurs. Cependant, le speedup reste croissant, ce qui t√©moigne de l'efficacit√© de la parall√©lisation.

Avec 8 c≈ìurs physiques (2 c≈ìurs logiques chacun), l'impl√©mentation atteint une performance √©quivalente √† environ 11 c≈ìurs logiques, confirmant l'efficacit√© de la parall√©lisation dans ce code.

#### Tableau des r√©sultats de scalabilit√© faible

Les tests de scalabilit√© faible ont √©galement √©t√© r√©alis√©s, avec les r√©sultats suivants :

| Nombre de c≈ìurs | Points lanc√©s | Points par c≈ìur | Temps d'ex√©cution (ms) | Approximation de œÄ | Erreur       |
|-----------------|---------------|-----------------|------------------------|--------------------|--------------|
| 1               | 1,000,000     | 1,000,000       | 63.0                   | 3.1423984          | 2.56 √ó 10‚Åª‚Å¥ |
| 2               | 2,000,000     | 1,000,000       | 56.2                   | 3.1412836          | 9.84 √ó 10‚Åª‚Åµ |
| 4               | 4,000,000     | 1,000,000       | 64.6                   | 3.1418568          | 8.41 √ó 10‚Åª‚Åµ |
| 8               | 8,000,000     | 1,000,000       | 109.6                  | 3.1417325          | 4.45 √ó 10‚Åª‚Åµ |
| 16              | 16,000,000    | 1,000,000       | 156.4                  | 3.14142355         | 5.38 √ó 10‚Åª‚Åµ |
| 1               | 10,000,000    | 10,000,000      | 530.8                  | 3.14203488         | 1.41 √ó 10‚Åª‚Å¥ |
| 2               | 20,000,000    | 10,000,000      | 552.0                  | 3.14175008         | 5.01 √ó 10‚Åª‚Åµ |
| 4               | 40,000,000    | 10,000,000      | 534.4                  | 3.14163064         | 1.21 √ó 10‚Åª‚Åµ |
| 8               | 80,000,000    | 10,000,000      | 579.4                  | 3.14150764         | 2.71 √ó 10‚Åª‚Åµ |
| 16              | 160,000,000   | 10,000,000      | 776.2                  | 3.1415749          | 5.65 √ó 10‚Åª‚Å∂ |
| 1               | 100,000,000   | 100,000,000     | 4662.4                 | 3.141613336        | 6.58 √ó 10‚Åª‚Å∂ |
| 2               | 200,000,000   | 100,000,000     | 5105.6                 | 3.14153096         | 1.96 √ó 10‚Åª‚Åµ |
| 4               | 400,000,000   | 100,000,000     | 5256.0                 | 3.141570872        | 6.93 √ó 10‚Åª‚Å∂ |
| 8               | 800,000,000   | 100,000,000     | 7791.6                 | 3.141575614        | 5.42 √ó 10‚Åª‚Å∂ |
| 16              | 1,600,000,000 | 100,000,000     | 8710.6                 | 3.141579264        | 4.26 √ó 10‚Åª‚Å∂ |

#### Courbe de scalabilit√© faible

---

![speedup pi](./images/scalabilite_faible_Pi.java_100000000.png)

---


Le speedup d√©cro√Æt lentement √† mesure que le nombre de processeurs augmente. Bien que cette d√©croissance soit mod√©r√©e par rapport √† l'Assignment102, le speedup passe de 1 (avec un seul processeur) √† environ 0,75 avec 16 processeurs.

Cette baisse indique que le code Pi.java perd en efficacit√© parall√®le avec l'augmentation des ressources disponibles, mais cette perte reste contenue. Compar√© √† Assignment102, o√π la scalabilit√© chute bien plus rapidement, les r√©sultats de Pi.java restent globalement satisfaisants.

# V. Mise en ≈ìuvre en m√©moire distribu√©e

Les analyses pr√©c√©dentes ont montr√© que le paradigme Master/Worker offre une meilleure parall√©lisation par rapport √† l‚Äôapproche d'Assignment102. Nous allons maintenant transposer cet algorithme sur une architecture √† m√©moire distribu√©e.

### Paradigme Master/Worker et Architecture Client/Serveur

Le mod√®le Master/Worker est souvent oppos√© au paradigme Client/Serveur. Dans le cadre de cette architecture, le Ma√Ætre agit comme un client, tandis que les Workers jouent le r√¥le de serveurs. Nous allons impl√©menter cette approche sur une architecture distribu√©e avec des √©changes via des sockets Java. Bien que le code de base fonctionne d√©j√†, la partie calcul de Pi n‚Äôest pas encore int√©gr√©e.

### Diagramme d'ex√©cution du code

Dans cette architecture, un *Master Socket* initialise l‚Äôexp√©rience Monte Carlo. Il r√©partit les t√¢ches en attribuant un nombre de points √† chaque *Worker Socket*. Chaque Worker calcule les r√©sultats (actuellement une valeur approximative) et renvoie ces r√©sultats au Master.

### Diagramme de classes UML

Les √©changes entre le *Master Socket* et les *Worker Sockets* s'appuient sur les classes de la biblioth√®que `java.net`. Les flux de donn√©es sont g√©r√©s par `InputStreamReader` et `OutputStreamWriter`, tandis que `PrintWriter` et `BufferedWriter` sont utilis√©s pour envoyer des messages, et `BufferedReader` pour les lire.

Pour ex√©cuter le programme, il est n√©cessaire de lancer diff√©rentes instances de `WorkerSocket` et `MasterSocket`. Lors du lancement de `WorkerSocket`, il faut sp√©cifier le port pour l‚Äôenvoi et la r√©ception des flux de donn√©es. De son c√¥t√©, le *MasterSocket* demande √† l'utilisateur d'entrer le nombre de Workers et leurs ports.

**Note :** Les ports saisis lors du lancement du *Master* n'ont pas d'importance, car le programme affecte automatiquement les ports 25545, 25546, etc., jusqu'√† avoir suffisamment de ports pour chaque Worker.

### A. Impl√©mentation du calcul par m√©thode

#### Nouvelle classe WorkerSocket

Une m√©thode `performMonteCarloComputation` a √©t√© ajout√©e √† la classe `WorkerSocket`. Elle est appel√©e lorsque le Worker re√ßoit une demande de calcul, ce qui lui permet de traiter sa part des points Monte Carlo et de renvoyer le r√©sultat au Master.

### B. Impl√©mentation du calcul en utilisant Pi.java

Bien qu‚Äôune m√©thode d√©di√©e au calcul Monte Carlo soit fonctionnelle, une approche plus int√©ressante consiste √† r√©utiliser l'algorithme Pi.java. Ainsi, la classe *Master* de Pi.java est int√©gr√©e directement dans le `WorkerSocket`.

Cela donne lieu √† une architecture Master/Worker multi-niveaux :

1. **Niveau 1 :** Un *Master Socket* r√©partit les t√¢ches entre plusieurs `Worker Sockets` sur une architecture √† m√©moire distribu√©e.
2. **Niveau 2 :** Chaque `Worker Socket` devient un Master d'une architecture Master/Worker √† m√©moire partag√©e, en int√©grant l'impl√©mentation de Pi.java.

Ce mod√®le, appel√© Programmation Multi-Niveaux, exploite les avantages de deux types de parall√©lisme : 
- Le parall√©lisme sur m√©moire distribu√©e au niveau sup√©rieur.
- Le parall√©lisme sur m√©moire partag√©e au niveau inf√©rieur.
